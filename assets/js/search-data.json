{
  
    
        "post0": {
            "title": "Title",
            "content": "# !pip install transformers[sentencepiece] # !pip install nb_black . %load_ext nb_black . import pandas as pd from pathlib import Path path = Path(&quot;/home/ec2-user/SageMaker/&quot;) df = pd.read_csv(path / &quot;df_raw.csv&quot;) df.dropna(inplace=True) . df.describe(include=&quot;object&quot;) . gut_codes transcript_text . count 6268 | 6268 | . unique 505 | 2927 | . top K12.MA-MA7-RPS-A.03 | Explore the Relationship Among Fractions, Deci... | . freq 82 | 60 | . gutcode_to_code = {val: idx for idx, val in enumerate(pd.unique(df.gut_codes))} . df[&quot;labels&quot;] = df[&quot;gut_codes&quot;].map(lambda x: gutcode_to_code[x]) . . df . Unnamed: 0 gut_codes transcript_text labels . 0 0 | K12.MA-MA6-RPS-A.02 | Ratio Worksheets | Simple Ratio Worksheets Thi... | 0 | . 1 1 | K12.MA-A1-A-REI.C.01 | Systems of Linear Equations: Solving by Additi... | 1 | . 2 2 | K12.MA-A1-F-IF.B.01 | Increasing and Decreasing Functions math, math... | 2 | . 3 3 | K12.MA-A2-F-IF.B.01 | Increasing and Decreasing Functions math, math... | 3 | . 4 4 | K12.MA-A1-F-LE.A.01.03 | Increasing and Decreasing Functions math, math... | 4 | . ... ... | ... | ... | ... | . 8472 8472 | K12.MA-A1-A-SSE.B.01.03 | Illustrative Mathematics Providing instruction... | 26 | . 8473 8473 | K12.MA-A1-A-REI.B.02.02 | 3 Ways to Solve Quadratic Equations - wikiHow ... | 85 | . 8474 8474 | K12.MA-GEO-SP-CP.A.02 | Illustrative Mathematics Providing instruction... | 115 | . 8475 8475 | K12.MA-GEO-SP-CP.A.03 | Illustrative Mathematics Providing instruction... | 113 | . 8476 8476 | K12.MA-MA6-TNS-C.02.02 | Diplodocus Using Coordinates math, maths, math... | 287 | . 6268 rows × 4 columns . df[&quot;text&quot;] = &quot;TEXT1: &quot; + df.transcript_text . from datasets import Dataset, DatasetDict dataset = Dataset.from_pandas(df) . dataset . Dataset({ features: [&#39;Unnamed: 0&#39;, &#39;gut_codes&#39;, &#39;transcript_text&#39;, &#39;labels&#39;, &#39;text&#39;, &#39;__index_level_0__&#39;], num_rows: 6268 }) . dataset = dataset.remove_columns([&quot;Unnamed: 0&quot;, &quot;transcript_text&quot;, &quot;__index_level_0__&quot;]) . dataset . Dataset({ features: [&#39;gut_codes&#39;, &#39;labels&#39;, &#39;text&#39;], num_rows: 6268 }) . dataset = dataset.train_test_split(0.10, seed=42) . dataset . DatasetDict({ train: Dataset({ features: [&#39;gut_codes&#39;, &#39;labels&#39;, &#39;text&#39;], num_rows: 5641 }) test: Dataset({ features: [&#39;gut_codes&#39;, &#39;labels&#39;, &#39;text&#39;], num_rows: 627 }) }) . from transformers import DistilBertTokenizerFast tokenizer = DistilBertTokenizerFast.from_pretrained(&quot;distilbert-base-uncased&quot;) . train_encodings = tokenizer( dataset[&quot;train&quot;][&quot;text&quot;], truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;, ) val_encodings = tokenizer( dataset[&quot;test&quot;][&quot;text&quot;], truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;, ) . import torch class BankingDataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&quot;labels&quot;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) . train_dataset = BankingDataset(train_encodings, dataset[&quot;train&quot;][&quot;labels&quot;]) val_dataset = BankingDataset(val_encodings, dataset[&quot;test&quot;][&quot;labels&quot;]) # test_dataset = IMDbDataset(test_encodings, test_labels) . from transformers import TrainingArguments, Trainer from transformers import AutoModelForSequenceClassification, AutoTokenizer . bs = 16 epochs = 4 lr = 8e-5 . args = TrainingArguments( &quot;outputs&quot;, learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type=&quot;cosine&quot;, fp16=True, evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=bs, per_device_eval_batch_size=bs * 2, num_train_epochs=epochs, weight_decay=0.01, report_to=&quot;none&quot;, ) . def compute_metrics(eval_pred): import numpy as np from datasets import load_metric metric1 = load_metric(&quot;precision&quot;) metric2 = load_metric(&quot;recall&quot;) metric3 = load_metric(&quot;f1&quot;) metric4 = load_metric(&quot;accuracy&quot;) logits, labels = eval_pred predictions = np.argmax(logits, axis=-1) precision = metric1.compute( predictions=predictions, references=labels, average=&quot;micro&quot; )[&quot;precision&quot;] recall = metric2.compute( predictions=predictions, references=labels, average=&quot;micro&quot; )[&quot;recall&quot;] f1 = metric3.compute(predictions=predictions, references=labels, average=&quot;micro&quot;)[ &quot;f1&quot; ] accuracy = metric4.compute(predictions=predictions, references=labels)[&quot;accuracy&quot;] return {&quot;precision&quot;: precision, &quot;recall&quot;: recall, &quot;f1&quot;: f1, &quot;accuracy&quot;: accuracy} . model = AutoModelForSequenceClassification.from_pretrained( &quot;distilbert-base-uncased&quot;, num_labels=505 ) trainer = Trainer( model, args, train_dataset=train_dataset, eval_dataset=val_dataset, tokenizer=tokenizer, compute_metrics=compute_metrics, ) # model = AutoModelForSequenceClassification.from_pretrained( # &quot;distilbert-base-uncased&quot;, num_labels=505 # ) # trainer = Trainer( # model, # args, # train_dataset=train_dataset, # eval_dataset=val_dataset, # tokenizer=tokenizer, # ) . Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: [&#39;vocab_layer_norm.weight&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_layer_norm.bias&#39;, &#39;vocab_transform.weight&#39;, &#39;vocab_projector.bias&#39;, &#39;vocab_projector.weight&#39;] - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: [&#39;pre_classifier.bias&#39;, &#39;pre_classifier.weight&#39;, &#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Using cuda_amp half precision backend . trainer.train() . /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 5641 Num Epochs = 4 Instantaneous batch size per device = 16 Total train batch size (w. parallel, distributed &amp; accumulation) = 16 Gradient Accumulation steps = 1 Total optimization steps = 1412 /tmp/ipykernel_27360/166799662.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} You&#39;re using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding. . . [1412/1412 07:19, Epoch 4/4] Epoch Training Loss Validation Loss Precision Recall F1 Accuracy . 1 | No log | 5.445369 | 0.039872 | 0.039872 | 0.039872 | 0.039872 | . 2 | 5.716600 | 4.626223 | 0.074960 | 0.074960 | 0.074960 | 0.074960 | . 3 | 4.386700 | 4.218055 | 0.105263 | 0.105263 | 0.105263 | 0.105263 | . 4 | 4.386700 | 4.148781 | 0.116427 | 0.116427 | 0.116427 | 0.116427 | . &lt;/div&gt; &lt;/div&gt; ***** Running Evaluation ***** Num examples = 627 Batch size = 32 /tmp/ipykernel_27360/235686084.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use &#39;evaluate.load&#39; instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate metric1 = load_metric(&#34;precision&#34;) /tmp/ipykernel_27360/166799662.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} Saving model checkpoint to outputs/checkpoint-500 Configuration saved in outputs/checkpoint-500/config.json Model weights saved in outputs/checkpoint-500/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json /tmp/ipykernel_27360/166799662.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} ***** Running Evaluation ***** Num examples = 627 Batch size = 32 /tmp/ipykernel_27360/166799662.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} Saving model checkpoint to outputs/checkpoint-1000 Configuration saved in outputs/checkpoint-1000/config.json Model weights saved in outputs/checkpoint-1000/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json /tmp/ipykernel_27360/166799662.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} ***** Running Evaluation ***** Num examples = 627 Batch size = 32 /tmp/ipykernel_27360/166799662.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} ***** Running Evaluation ***** Num examples = 627 Batch size = 32 Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=1412, training_loss=4.686880657422981, metrics={&#39;train_runtime&#39;: 440.6226, &#39;train_samples_per_second&#39;: 51.209, &#39;train_steps_per_second&#39;: 3.205, &#39;total_flos&#39;: 3015806532464640.0, &#39;train_loss&#39;: 4.686880657422981, &#39;epoch&#39;: 4.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; import numpy as np preds = trainer.predict(val_dataset).predictions.astype(float) predictions = np.argmax(preds, axis=-1) actual = (val_dataset).labels y_pred = predictions . ***** Running Prediction ***** Num examples = 627 Batch size = 32 /tmp/ipykernel_27360/166799662.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} . def check_top_5(idx): return True if actual[idx] in preds[idx].argsort()[-5:][::-1] else False . true_count = 0 false_count = 0 for i in range(0, len(y_pred)): if check_top_5(i): true_count += 1 else: false_count += 1 print(true_count, false_count) . 230 397 . 230 / (230 + 397) * 100 . 36.68261562998405 . def check_top_10(idx): return True if actual[idx] in preds[idx].argsort()[-10:][::-1] else False . true_count = 0 false_count = 0 for i in range(0, len(y_pred)): if check_top_10(i): true_count += 1 else: false_count += 1 print(true_count, false_count) . 319 308 . 319 / (319 + 308) * 100 . 50.877192982456144 . &lt;/div&gt; .",
            "url": "https://gauravadlakha1509.github.io/sample-ds-blog/2022/10/03/kw_HugingFace-nucleus.html",
            "relUrl": "/2022/10/03/kw_HugingFace-nucleus.html",
            "date": " • Oct 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://gauravadlakha1509.github.io/sample-ds-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gauravadlakha1509.github.io/sample-ds-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gauravadlakha1509.github.io/sample-ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gauravadlakha1509.github.io/sample-ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}